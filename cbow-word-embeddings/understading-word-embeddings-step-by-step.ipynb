{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"understading-word-embeddings-step-by-step.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Bt3o8wo58r3S","colab_type":"text"},"source":["# **Understading CBOW (*continuous bag of words*) Word Embeddings Step-by-step**\n","\n","In this notebook, we'll try out a small example to create word embeddings.\n","\n","There are two main parts: data preparation, and the continuous bag-of-words (CBOW) model.\n","\n","To get started, import and initialize all the libraries we will need."]},{"cell_type":"code","metadata":{"id":"vO44Kvj28r3T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1594741136977,"user_tz":-480,"elapsed":7727,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4118d43f-5711-430c-e710-435f1025aae5"},"source":["import sys\n","!{sys.executable} -m pip install emoji"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 16.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=2b6487fb4342cb2cb56d56c710426057114e6ec5c4755ec8a175786b80ff72c3\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zlTQcxzG-vwy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"status":"ok","timestamp":1594741183126,"user_tz":-480,"elapsed":16892,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"7b70a125-69f3-4727-8e0c-dcdeb542a5b5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iSoGUD58-sT6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741205877,"user_tz":-480,"elapsed":2177,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["! cp '/content/drive/My Drive/Colab Notebooks/NLP-with-probabilistic-models/cbow-word-embeddings/utils2.py' '/content'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8nzmlpRG8r3W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594741213397,"user_tz":-480,"elapsed":2768,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"ea1dde03-baef-4b98-c971-847586ce052a"},"source":["import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import emoji\n","import numpy as np\n","\n","from utils2 import get_dict\n","\n","# download pre-trained Punkt tokenizer for English\n","nltk.download('punkt')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"O_U2h22y8r3Y","colab_type":"text"},"source":["## **Data preparation**"]},{"cell_type":"markdown","metadata":{"id":"H6hVf2Kw8r3Z","colab_type":"text"},"source":["In the data preparation phase, we will start with a corpus of text, and:\n","\n","- Clean and tokenize the corpus.\n","\n","- Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n","\n","- Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model."]},{"cell_type":"markdown","metadata":{"id":"tvKMxGe88r3Z","colab_type":"text"},"source":["#### **Cleaning and tokenization**\n","\n","To understand the cleaning and tokenization process, we will consider a small example (corpus) that contains emojis and various punctuation signs."]},{"cell_type":"code","metadata":{"id":"sWbsQL4l8r3a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741216511,"user_tz":-480,"elapsed":686,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["corpus = 'Who ❤️ \"word embeddings\" in 2020? I do!!!'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0W0jb8E8r3c","colab_type":"text"},"source":["First, we will replace all interrupting punctuation signs — such as commas and exclamation marks — with periods."]},{"cell_type":"code","metadata":{"id":"VZmyZ0DE8r3c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594741218177,"user_tz":-480,"elapsed":902,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4677cb8c-b34d-4498-c873-838bc1ecf712"},"source":["print(f'Corpus:  {corpus}')\n","data = re.sub(r'[,!?;-]+', '.', corpus)\n","print(f'After cleaning punctuation:  {data}')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Corpus:  Who ❤️ \"word embeddings\" in 2020? I do!!!\n","After cleaning punctuation:  Who ❤️ \"word embeddings\" in 2020. I do.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_yeJ-DSa8r3e","colab_type":"text"},"source":["Next, we will use NLTK's tokenization engine to split the corpus into individual tokens."]},{"cell_type":"code","metadata":{"id":"4-RVV1wO8r3f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594741220438,"user_tz":-480,"elapsed":764,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"7d0ed6ec-4a95-458d-bbae-e30a86569e6e"},"source":["print(f'Initial string:  {data}')\n","data = nltk.word_tokenize(data)\n","print(f'After tokenization:  {data}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Initial string:  Who ❤️ \"word embeddings\" in 2020. I do.\n","After tokenization:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SCLNey4r8r3h","colab_type":"text"},"source":["Finally, we will get rid of numbers and punctuation other than periods, and convert all the remaining tokens to lowercase."]},{"cell_type":"code","metadata":{"id":"ZCPfDAYg8r3h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594741224137,"user_tz":-480,"elapsed":732,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"00bdc50f-e877-4611-baba-6634ec08bf5c"},"source":["print(f'Initial list of tokens:  {data}')\n","data = [ ch.lower() for ch in data\n","         if ch.isalpha()\n","         or ch == '.'\n","         or emoji.get_emoji_regexp().search(ch)\n","       ]\n","print(f'After cleaning:  {data}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Initial list of tokens:  ['Who', '❤️', '``', 'word', 'embeddings', \"''\", 'in', '2020', '.', 'I', 'do', '.']\n","After cleaning:  ['who', '❤️', 'word', 'embeddings', 'in', '.', 'i', 'do', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ppDttT9q8r3k","colab_type":"text"},"source":["Note that the heart emoji is considered as a token just like any normal word.\n","\n","Now let's streamline the cleaning and tokenization process by wrapping the previous steps in a function."]},{"cell_type":"code","metadata":{"id":"5n2L0sBF8r3l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741227940,"user_tz":-480,"elapsed":725,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def tokenize(corpus):\n","    '''\n","    Returns the cleaned and tokenized list of input data.\n","    '''\n","    data = re.sub(r'[,!?;-]+', '.', corpus)\n","    data = nltk.word_tokenize(data)  # tokenize string to words\n","    data = [ ch.lower() for ch in data\n","             if ch.isalpha()\n","             or ch == '.'\n","             or emoji.get_emoji_regexp().search(ch)\n","           ]\n","    return data"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2YJGQYq48r3n","colab_type":"text"},"source":["Let's apply this function to the corpus that we'll be working on in the rest of this notebook: \"I am happy because I am learning continuous-bag-of-words word embeddings.\""]},{"cell_type":"code","metadata":{"id":"fOUF_Xbg8r3n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594744998223,"user_tz":-480,"elapsed":766,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"ae5a2032-a929-4f38-be58-146730619f5e"},"source":["corpus = 'I am happy because I am learning'\n","print(f'Corpus:  {corpus}')\n","words = tokenize(corpus)\n","print(f'Words (tokens):  {words}')"],"execution_count":76,"outputs":[{"output_type":"stream","text":["Corpus:  I am happy because I am learning\n","Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oxfXTjZb8r3r","colab_type":"text"},"source":["#### **Sliding window of words**"]},{"cell_type":"markdown","metadata":{"id":"jDTXQJuz8r3s","colab_type":"text"},"source":["Now that we have transformed the corpus into a list of clean tokens, we can slide a window of words across this list. For each window we can extract a center word and the context words.\n","\n","The `get_windows` function in the next cell helps us with that."]},{"cell_type":"code","metadata":{"id":"h4MBEuK68r3t","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741308511,"user_tz":-480,"elapsed":698,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def get_windows(words, C):\n","    i = C\n","    while i < len(words) - C:\n","        center_word = words[i]\n","        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n","        yield context_words, center_word\n","        i += 1"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SWywtXHE8r3v","colab_type":"text"},"source":["The first argument of this function is a list of words (or tokens). The second argument, `C`, is the context half-size. For a given center word, the context words are made of `C` words to the left and `C` words to the right of the center word.\n","\n","Below is an illustation on how we can use this function to extract context words and center words from a list of tokens. These context and center words will make up the training set that we will use to train the CBOW model."]},{"cell_type":"code","metadata":{"id":"OjasVFvn8r3w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745011635,"user_tz":-480,"elapsed":818,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"49d7bcf9-6b81-4f5f-c6ac-7cccca9634db"},"source":["for x, y in get_windows(\n","            ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n","            2\n","        ):\n","    print(f'{x}\\t{y}')"],"execution_count":77,"outputs":[{"output_type":"stream","text":["['i', 'am', 'because', 'i']\thappy\n","['am', 'happy', 'i', 'am']\tbecause\n","['happy', 'because', 'am', 'learning']\ti\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PqqNjiXa8r3y","colab_type":"text"},"source":["The first example of the training set is made of:\n","\n","- the context words \"i\", \"am\", \"because\", \"i\",\n","\n","- and the center word to be predicted: \"happy\".\n"]},{"cell_type":"code","metadata":{"id":"az2SywOR8r3y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745020535,"user_tz":-480,"elapsed":788,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"598556b0-0c0c-4f1e-87a0-3af9d111651e"},"source":["# let's try and change the context half-size\n","for x, y in get_windows(tokenize(\"I am happy because I am learning\"), 1):\n","    print(f'{x}\\t{y}')"],"execution_count":78,"outputs":[{"output_type":"stream","text":["['i', 'happy']\tam\n","['am', 'because']\thappy\n","['happy', 'i']\tbecause\n","['because', 'am']\ti\n","['i', 'learning']\tam\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K6pUrt9q8r30","colab_type":"text"},"source":["#### **Transforming words into vectors for the training set**"]},{"cell_type":"markdown","metadata":{"id":"DmapasiL8r31","colab_type":"text"},"source":["To finish preparing the training set, we need to transform the context words and center words into vectors.\n","\n","#### Mapping words to indices and indices to words\n","\n","The center words will be represented as one-hot vectors, and the vectors that represent context words are also based on one-hot vectors.\n","\n","To create one-hot word vectors, we can start by mapping each unique word to a unique integer (or index). We will use a helper function, `get_dict` from **utils2.py**, that creates a Python dictionary that maps words to integers and back."]},{"cell_type":"code","metadata":{"id":"d_L_D6g48r32","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745029354,"user_tz":-480,"elapsed":849,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["word2Ind, Ind2word = get_dict(words)"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hXJ_U788r36","colab_type":"text"},"source":["Here's the dictionary that maps words to numeric indices."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tdl9MF6E8r36","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745030195,"user_tz":-480,"elapsed":706,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"6a0fddfd-24c0-49e0-aa05-5aa732ef4d13"},"source":["word2Ind"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"cSXc1MVo8r39","colab_type":"text"},"source":["We can use this dictionary to get the index of a word."]},{"cell_type":"code","metadata":{"id":"S-Wcof_Q8r39","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745035028,"user_tz":-480,"elapsed":834,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"58df799a-966a-4322-fa5f-71d84418e076"},"source":["print(\"Index of the word 'i':  \",word2Ind['i'])"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Index of the word 'i':   3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gGnQZAc08r4A","colab_type":"text"},"source":["And conversely, here's the dictionary that maps indices to words."]},{"cell_type":"code","metadata":{"id":"9zTyOQba8r4A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745036512,"user_tz":-480,"elapsed":747,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"2de046ed-45c5-45b6-ebd9-aa3a67d4ccef"},"source":["Ind2word"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: 'am', 1: 'because', 2: 'happy', 3: 'i', 4: 'learning'}"]},"metadata":{"tags":[]},"execution_count":82}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"yPNptzR18r4C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745039662,"user_tz":-480,"elapsed":1015,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"082372a6-8df8-4e1e-e2e9-c389762bd911"},"source":["print(\"Word which has index 2:  \",Ind2word[2] )"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Word which has index 2:   happy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x7DIpZRP8r4E","colab_type":"text"},"source":["Finally, in order to get the size of the vocabulary of our corpus (the number of different words making up the corpus), we can simply get the length of either of these dictionaries."]},{"cell_type":"code","metadata":{"id":"ZQU8f-wo8r4F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745040515,"user_tz":-480,"elapsed":485,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"407047a2-b6e3-4d0d-e288-f43210bf58a0"},"source":["V = len(word2Ind)\n","print(\"Size of vocabulary: \", V)"],"execution_count":84,"outputs":[{"output_type":"stream","text":["Size of vocabulary:  5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VDb6ovOm8r4H","colab_type":"text"},"source":["#### Getting one-hot word vectors\n","\n","We can easily convert an integer, $n$, into a one-hot vector by simply assigning **1** to index $n$ and keeping the rest indices as **0**.\n","\n","Consider the word \"happy\". First, retrieve its numeric index."]},{"cell_type":"code","metadata":{"id":"01eudi5p8r4H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745045252,"user_tz":-480,"elapsed":864,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"69e96bc7-b8bd-4f90-b5ba-993033afc464"},"source":["n = word2Ind['happy']\n","n"],"execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"markdown","metadata":{"id":"WeYHye5H8r4J","colab_type":"text"},"source":["Now create a vector with the size of the vocabulary, and fill it with zeros."]},{"cell_type":"code","metadata":{"id":"q1lkPBAt8r4J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745046378,"user_tz":-480,"elapsed":640,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"5db21cb9-e6a1-443a-8b61-bf393b61e7c5"},"source":["center_word_vector = np.zeros(V)\n","center_word_vector"],"execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"gjoh5dS38r4L","colab_type":"text"},"source":["We can confirm that the vector has the right size."]},{"cell_type":"code","metadata":{"id":"ncEN_eGX8r4L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745049228,"user_tz":-480,"elapsed":798,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"fee0b168-3b8f-4dd2-b817-a7aae92e3e85"},"source":["len(center_word_vector) == V"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"1zl01jH18r4O","colab_type":"text"},"source":["Next, replace the 0 of the $n$-th element with a 1."]},{"cell_type":"code","metadata":{"id":"FzVWNKAU8r4O","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745050545,"user_tz":-480,"elapsed":822,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["center_word_vector[n] = 1"],"execution_count":88,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHuwzaTz8r4Q","colab_type":"text"},"source":["And we have our one-hot word vector."]},{"cell_type":"code","metadata":{"id":"n42srD5W8r4Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745051307,"user_tz":-480,"elapsed":548,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"293389e9-f649-453a-cb0d-127d5080cb6c"},"source":["center_word_vector"],"execution_count":89,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 1., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"o58G6itZ8r4S","colab_type":"text"},"source":["We will now group all of these steps in a convenient function, which takes as parameters: a word to be encoded, a dictionary that maps words to indices, and the size of the vocabulary."]},{"cell_type":"code","metadata":{"id":"rEVBlTNa8r4S","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594741907814,"user_tz":-480,"elapsed":711,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def word_to_one_hot_vector(word, word2Ind, V):\n","    one_hot_vector = np.zeros(V)\n","    one_hot_vector[word2Ind[word]] = 1\n","    \n","    return one_hot_vector"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JFzx5iuO8r4U","colab_type":"text"},"source":["Let's check that it works as intended."]},{"cell_type":"code","metadata":{"id":"2RBSPMEa8r4U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745059556,"user_tz":-480,"elapsed":854,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"82aeb461-afbe-427b-e2fe-9675e422cae0"},"source":["word_to_one_hot_vector('happy', word2Ind, V)"],"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 1., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":90}]},{"cell_type":"markdown","metadata":{"id":"d_seCGTG8r4a","colab_type":"text"},"source":["#### Getting context word vectors"]},{"cell_type":"markdown","metadata":{"id":"TTbOZAzx8r4a","colab_type":"text"},"source":["To create the vectors that represent context words, we will simply calculate the average of the one-hot vectors representing the individual words.\n","\n","Let's start with a list of context words."]},{"cell_type":"code","metadata":{"id":"ZNDjcKyK8r4a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745069553,"user_tz":-480,"elapsed":899,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["context_words = ['i', 'am', 'because', 'i']"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qtIpk6Z8r4c","colab_type":"text"},"source":["Using Python's list comprehension construct and the `word_to_one_hot_vector` function that we created above, we can create a list of one-hot vectors representing each of the context words."]},{"cell_type":"code","metadata":{"id":"e36hx12c8r4d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1594745071007,"user_tz":-480,"elapsed":725,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"d35b1aa2-a7b9-4e5c-ae01-07c09804455f"},"source":["context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n","context_words_vectors"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([0., 0., 0., 1., 0.]),\n"," array([1., 0., 0., 0., 0.]),\n"," array([0., 1., 0., 0., 0.]),\n"," array([0., 0., 0., 1., 0.])]"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"markdown","metadata":{"id":"yb3-8nSP8r4f","colab_type":"text"},"source":["And we can now simply get the average of these vectors using numpy's `mean` function, to get the vector representation of the context words."]},{"cell_type":"code","metadata":{"id":"XEghORG78r4f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745072455,"user_tz":-480,"elapsed":763,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"07dce9f5-90e9-406f-e4ee-a025d3b7496e"},"source":["np.mean(context_words_vectors, axis=0)"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.25, 0.25, 0.  , 0.5 , 0.  ])"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"markdown","metadata":{"id":"p8u-ZVbB8r4h","colab_type":"text"},"source":["Now, we will create a `context_words_to_vector` function that takes in a list of context words, a word-to-index dictionary, and a vocabulary size, and outputs the vector representation of the context words."]},{"cell_type":"code","metadata":{"id":"FUk_Qcow8r4h","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594742082344,"user_tz":-480,"elapsed":1528,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def context_words_to_vector(context_words, word2Ind, V):\n","    context_words_vectors = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n","    context_words_vectors = np.mean(context_words_vectors, axis=0)\n","    \n","    return context_words_vectors"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czGWLxZ28r4j","colab_type":"text"},"source":["Let's check that we obtain the same output as the manual approach above."]},{"cell_type":"code","metadata":{"id":"l_-NkMtz8r4j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745076906,"user_tz":-480,"elapsed":695,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"0f9c1b18-2777-4c8f-e4ae-4b660864b0c9"},"source":["context_words_to_vector(['i', 'am', 'because', 'i'], word2Ind, V)"],"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.25, 0.25, 0.  , 0.5 , 0.  ])"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"markdown","metadata":{"id":"3mvgDlx88r4p","colab_type":"text"},"source":["## **Building the training set**"]},{"cell_type":"markdown","metadata":{"id":"SpxrgLcV8r4p","colab_type":"text"},"source":["We can now combine the functions that we created in the previous sections, to build a training set for the CBOW model, starting from the following tokenized corpus."]},{"cell_type":"code","metadata":{"id":"989b06XT8r4q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745081574,"user_tz":-480,"elapsed":939,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"cf0b11e1-f6a2-4a27-a4db-817c574042ed"},"source":["words"],"execution_count":95,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'am', 'happy', 'because', 'i', 'am', 'learning']"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"markdown","metadata":{"id":"4HRCs26a8r4t","colab_type":"text"},"source":["To do this, we need to use the sliding window function (`get_windows`) to extract the context words and center words, and then convert these sets of words into a basic vector representation using `word_to_one_hot_vector` and `context_words_to_vector`."]},{"cell_type":"code","metadata":{"id":"0J9-FzWz8r4t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1594745082745,"user_tz":-480,"elapsed":494,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"6add6fb6-6035-4c0a-9b89-66a5d3832bb2"},"source":["for context_words, center_word in get_windows(words, 2):\n","    print(f'Context words:  {context_words} -> {context_words_to_vector(context_words, word2Ind, V)}')\n","    print(f'Center word:  {center_word} -> {word_to_one_hot_vector(center_word, word2Ind, V)}')\n","    print()"],"execution_count":96,"outputs":[{"output_type":"stream","text":["Context words:  ['i', 'am', 'because', 'i'] -> [0.25 0.25 0.   0.5  0.  ]\n","Center word:  happy -> [0. 0. 1. 0. 0.]\n","\n","Context words:  ['am', 'happy', 'i', 'am'] -> [0.5  0.   0.25 0.25 0.  ]\n","Center word:  because -> [0. 1. 0. 0. 0.]\n","\n","Context words:  ['happy', 'because', 'am', 'learning'] -> [0.25 0.25 0.25 0.   0.25]\n","Center word:  i -> [0. 0. 0. 1. 0.]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FLSFFuw28r4x","colab_type":"text"},"source":["Here, we have just performed a single iteration of training using a single example, but in real-life scenario we train the CBOW model using several iterations and batches of example.\n","Below is a glimpse of how we would use a Python generator function to make it easier to iterate over a set of examples."]},{"cell_type":"code","metadata":{"id":"wGsTYKMA8r4y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745086302,"user_tz":-480,"elapsed":694,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def get_training_example(words, C, word2Ind, V):\n","    for context_words, center_word in get_windows(words, C):\n","        yield context_words_to_vector(context_words, word2Ind, V), word_to_one_hot_vector(center_word, word2Ind, V)"],"execution_count":97,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"We0MZRN48r40","colab_type":"text"},"source":["The output of this function can be iterated on to get successive context word vectors and center word vectors, as demonstrated in the next cell."]},{"cell_type":"code","metadata":{"id":"W1gJaA7M8r40","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1594745088650,"user_tz":-480,"elapsed":801,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"a263a0a1-d0fa-4a84-8a4e-4e487814f1fb"},"source":["for context_words_vector, center_word_vector in get_training_example(words, 2, word2Ind, V):\n","    print(f'Context words vector:  {context_words_vector}')\n","    print(f'Center word vector:  {center_word_vector}')\n","    print()"],"execution_count":98,"outputs":[{"output_type":"stream","text":["Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n","Center word vector:  [0. 0. 1. 0. 0.]\n","\n","Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n","Center word vector:  [0. 1. 0. 0. 0.]\n","\n","Context words vector:  [0.25 0.25 0.25 0.   0.25]\n","Center word vector:  [0. 0. 0. 1. 0.]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gL6H-dva8r43","colab_type":"text"},"source":["Our training set is ready!\n","\n","We can now move on to the CBOW model itself."]},{"cell_type":"markdown","metadata":{"id":"YJUy5izT8r44","colab_type":"text"},"source":["## **The continuous bag-of-words model**"]},{"cell_type":"markdown","metadata":{"id":"ZRcd-Uz-8r44","colab_type":"text"},"source":["The CBOW model is based on a neural network, the architecture of which looks like the figure below:\n","\n","<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://angqpdfr.coursera-apps.org/notebooks/Week4/cbow_model_architecture.png?1' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:917;height:337;\" /> </div>\n","\n","Next up, we will see:\n","\n","- The two activation functions used in the neural network we will design.\n","\n","- Forward propagation.\n","\n","- Cross-entropy loss.\n","\n","- Backpropagation.\n","\n","- Gradient descent.\n","\n","- Extracting the word embedding vectors from the weight matrices once the neural network has been trained."]},{"cell_type":"markdown","metadata":{"id":"IHi3Ihi78r45","colab_type":"text"},"source":["#### **Activation functions**"]},{"cell_type":"markdown","metadata":{"id":"4G2NwUKD8r45","colab_type":"text"},"source":["Let's start by implementing the activation functions, ReLU and softmax."]},{"cell_type":"markdown","metadata":{"id":"M13Xw3sA8r46","colab_type":"text"},"source":["#### ReLU"]},{"cell_type":"markdown","metadata":{"id":"0knLp8F08r46","colab_type":"text"},"source":["ReLU is used to calculate the values of the hidden layer, in the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_1} &= \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1} \\\\\n"," \\mathbf{h} &= \\mathrm{ReLU}(\\mathbf{z_1}) \\\\\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{"id":"C-1XFOW_8r47","colab_type":"text"},"source":["Let's fix a value for $\\mathbf{z_1}$ as a working example."]},{"cell_type":"code","metadata":{"id":"BvizUtUu8r47","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594742815693,"user_tz":-480,"elapsed":673,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"504ea0f3-3657-455b-8557-245561792e54"},"source":["np.random.seed(10)\n","z_1 = 10*np.random.rand(5, 1)-5\n","z_1"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2.71320643],\n","       [-4.79248051],\n","       [ 1.33648235],\n","       [ 2.48803883],\n","       [-0.01492988]])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"18LHksV-8r4-","colab_type":"text"},"source":["To get the ReLU of this vector, we want all the negative values to become zeros.\n","\n","First we will create a copy of this vector."]},{"cell_type":"code","metadata":{"id":"qaXlrjTI8r4-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594742832799,"user_tz":-480,"elapsed":776,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["h = z_1.copy()"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4F_DY7Td8r5B","colab_type":"text"},"source":["Now we will determine which of its values are negative."]},{"cell_type":"code","metadata":{"id":"5VZuh8mY8r5B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594742842902,"user_tz":-480,"elapsed":1529,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"c52f2db6-41d2-4f8e-c41d-6ff0e0f8046f"},"source":["h < 0"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[False],\n","       [ True],\n","       [False],\n","       [False],\n","       [ True]])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"k5U33aaz8r5D","colab_type":"text"},"source":["We can now simply set all of the values which are negative to 0."]},{"cell_type":"code","metadata":{"id":"h3BtoPQm8r5D","colab_type":"code","colab":{}},"source":["h[h < 0] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8NdpkSqm8r5E","colab_type":"text"},"source":["And that's it, we have the ReLU of $\\mathbf{z_1}$!"]},{"cell_type":"code","metadata":{"id":"OBHCwbNF8r5G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594742866894,"user_tz":-480,"elapsed":1530,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"d210d61e-1e04-4ac0-8aff-972561caec73"},"source":["h"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2.71320643],\n","       [-4.79248051],\n","       [ 1.33648235],\n","       [ 2.48803883],\n","       [-0.01492988]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"05vZ5A6z8r5H","colab_type":"text"},"source":["Now we will implement ReLU as a function."]},{"cell_type":"code","metadata":{"id":"hra0rIDG8r5I","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594742887273,"user_tz":-480,"elapsed":1565,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def relu(z):\n","    result = z.copy()\n","    result[result < 0] = 0\n","    \n","    return result"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"HOIOgzV58r5L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594742892498,"user_tz":-480,"elapsed":1490,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"93344e33-a40c-4a07-ad83-df5bbc7cf331"},"source":["z = np.array([[-1.25459881], [ 4.50714306], [ 2.31993942], [ 0.98658484], [-3.4398136 ]])\n","relu(z)"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        ],\n","       [4.50714306],\n","       [2.31993942],\n","       [0.98658484],\n","       [0.        ]])"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"ksX0Dw228r5N","colab_type":"text"},"source":["#### Softmax"]},{"cell_type":"markdown","metadata":{"id":"gUTfzC1E8r5O","colab_type":"text"},"source":["The second activation function that we will be using is softmax. This function is used to calculate the values of the output layer of the neural network, using the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}  \\\\\n"," \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2}) \\\\\n","\\end{align}\n","\n","To calculate softmax of a vector $\\mathbf{z}$, the $i$-th component of the resulting vector is given by:\n","\n","$$ \\textrm{softmax}(\\textbf{z})_i = \\frac{e^{z_i} }{\\sum\\limits_{j=1}^{V} e^{z_j} }   $$\n","\n","Let's work through an example."]},{"cell_type":"code","metadata":{"id":"cbaAkpYZ8r5O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594743261444,"user_tz":-480,"elapsed":675,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"630f224b-e15c-4bd9-b721-f84a75bccd3e"},"source":["z = np.array([9, 8, 11, 10, 8.5])\n","z"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 9. ,  8. , 11. , 10. ,  8.5])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"id":"TNTgzGoS8r5Q","colab_type":"text"},"source":["We'll need to calculate the exponentials of each element, both for the numerator and for the denominator."]},{"cell_type":"code","metadata":{"id":"OfnpYmQt8r5R","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594743270659,"user_tz":-480,"elapsed":727,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"843fc7c2-0c8f-4b93-9770-111d09bfa04a"},"source":["e_z = np.exp(z)\n","e_z"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 8103.08392758,  2980.95798704, 59874.1417152 , 22026.46579481,\n","        4914.7688403 ])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"BwLe88ub8r5T","colab_type":"text"},"source":["The denominator is equal to the sum of these exponentials."]},{"cell_type":"code","metadata":{"id":"0Lq4mOc98r5U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594743280242,"user_tz":-480,"elapsed":5297,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"f324df54-2f35-405e-f2b0-c0a5727931a7"},"source":["sum_e_z = np.sum(e_z)\n","sum_e_z"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["97899.41826492078"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"rF6C4ljy8r5V","colab_type":"text"},"source":["And the value of the first element of $\\textrm{softmax}(\\textbf{z})$ is given by:"]},{"cell_type":"code","metadata":{"id":"NGqCxBzq8r5W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594743280244,"user_tz":-480,"elapsed":2849,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"0be961ed-9b99-47e4-9454-536612c933c3"},"source":["e_z[0]/sum_e_z"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.08276947985173956"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"oyZHfd7c8r5X","colab_type":"text"},"source":["This is for one element. We will use numpy's vectorized operations to calculate the values of all the elements of the $\\textrm{softmax}(\\textbf{z})$ vector in one go.\n","\n","Let's Implement the softmax function."]},{"cell_type":"code","metadata":{"id":"iJzRuHtg8r5Y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594743315918,"user_tz":-480,"elapsed":949,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def softmax(z):\n","    e_z = np.exp(z)\n","    sum_e_z = np.sum(e_z)\n","    \n","    return e_z / sum_e_z"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"id":"47ANgInt8r5a","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594743319089,"user_tz":-480,"elapsed":773,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"6114d6ad-eff9-428b-a985-6732d0511c4f"},"source":["softmax([9, 8, 11, 10, 8.5])"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.08276948, 0.03044919, 0.61158833, 0.22499077, 0.05020223])"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"Y7y0DNqf8r5d","colab_type":"text"},"source":["### Dimensions: 1-D arrays vs 2-D column vectors\n","\n","Before moving on to implement forward propagation, backpropagation, and gradient descent, let's have a look at the dimensions of the vectors we've been handling until now.\n","\n","We will start with creating a vector of length $V$ filled with zeros."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Zmy1wXv48r5d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745120586,"user_tz":-480,"elapsed":728,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"91846487-c489-4ffa-b968-ba6e31daaffa"},"source":["x_array = np.zeros(V)\n","x_array"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"markdown","metadata":{"id":"3WALq6kg8r5e","colab_type":"text"},"source":["This is a 1-dimensional array, as revealed by the `.shape` property of the array."]},{"cell_type":"code","metadata":{"id":"XDavkNqZ8r5f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745122598,"user_tz":-480,"elapsed":729,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"226b6410-1fb8-4621-b22a-3e869febbd9a"},"source":["x_array.shape"],"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5,)"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"yk29f_2o8r5h","colab_type":"text"},"source":["To perform matrix multiplication in the next steps, we actually need our column vectors to be represented as a matrix with one column. In numpy, this matrix is represented as a 2-dimensional array.\n","\n","The easiest way to convert a 1D vector to a 2D column matrix is to set its `.shape` property to the number of rows and one column, as shown in the next cell."]},{"cell_type":"code","metadata":{"id":"BUzQtj2R8r5i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745126939,"user_tz":-480,"elapsed":735,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"11940465-a207-42d2-b4e8-6ce6be2692ff"},"source":["x_column_vector = x_array.copy()\n","x_column_vector.shape = (V, 1)  # alternatively ... = (x_array.shape[0], 1)\n","x_column_vector"],"execution_count":101,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.]])"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"markdown","metadata":{"id":"J7MlkkLo8r5j","colab_type":"text"},"source":["The shape of the resulting \"vector\" is:"]},{"cell_type":"code","metadata":{"id":"paSDyEZW8r5j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745130629,"user_tz":-480,"elapsed":803,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"f3066df2-5988-43ed-90db-5a2bd5d08363"},"source":["x_column_vector.shape"],"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, 1)"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"ZLqdLeXq8r5l","colab_type":"text"},"source":["So now we have a 5x1 matrix that we can use to perform standard matrix multiplication."]},{"cell_type":"markdown","metadata":{"id":"NxvdEJ4A8r5l","colab_type":"text"},"source":["#### **Forward propagation**"]},{"cell_type":"markdown","metadata":{"id":"esMATdfe8r5l","colab_type":"text"},"source":["Let's dive into the neural network itself, which is shown below with all the dimensions and formulas we need.\n","\n","<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='https://angqpdfr.coursera-apps.org/notebooks/Week4/cbow_model_dimensions_single_input.png?2' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:839;height:349;\" /></div>"]},{"cell_type":"markdown","metadata":{"id":"EQdVumpO8r5l","colab_type":"text"},"source":["We will set $N$ equal to 3. $N$ is a hyperparameter of the CBOW model that represents the size of the word embedding vectors, as well as the size of the hidden layer."]},{"cell_type":"code","metadata":{"id":"ftZhZ3-z8r5m","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594744123710,"user_tz":-480,"elapsed":747,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["N = 3"],"execution_count":53,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2HbGfvi8r5n","colab_type":"text"},"source":["### Initialization of the weights and biases"]},{"cell_type":"markdown","metadata":{"id":"FPApo8gb8r5n","colab_type":"text"},"source":["Before we start training the neural network, we need to initialize the weight matrices and bias vectors with random values."]},{"cell_type":"code","metadata":{"id":"mFhp0pNJ8r5o","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745156119,"user_tz":-480,"elapsed":766,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n","               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n","               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n","\n","W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n","               [ 0.08476603,  0.08123194,  0.1772054 ],\n","               [ 0.1871551 , -0.06107263, -0.1790735 ],\n","               [ 0.07055222, -0.02015138,  0.36107434],\n","               [ 0.33480474, -0.39423389, -0.43959196]])\n","\n","b1 = np.array([[ 0.09688219],\n","               [ 0.29239497],\n","               [-0.27364426]])\n","\n","b2 = np.array([[ 0.0352008 ],\n","               [-0.36393384],\n","               [-0.12775555],\n","               [-0.34802326],\n","               [-0.07017815]])"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-7V7Sr-r8r5p","colab_type":"text"},"source":["Let's check if the dimensions of these matrices match those shown in the figure above."]},{"cell_type":"code","metadata":{"id":"wSsJyrQ68r5q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1594745157867,"user_tz":-480,"elapsed":813,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"c7dcc069-c7cc-4d61-94d4-4cc239e44ff8"},"source":["print(f'V (vocabulary size): {V}')\n","print(f'N (embedding size / size of the hidden layer): {N}')\n","print(f'size of W1: {W1.shape} (NxV)')\n","print(f'size of b1: {b1.shape} (Nx1)')\n","print(f'size of W2: {W1.shape} (VxN)')\n","print(f'size of b2: {b2.shape} (Vx1)')"],"execution_count":104,"outputs":[{"output_type":"stream","text":["V (vocabulary size): 5\n","N (embedding size / size of the hidden layer): 3\n","size of W1: (3, 5) (NxV)\n","size of b1: (3, 1) (Nx1)\n","size of W2: (3, 5) (VxN)\n","size of b2: (5, 1) (Vx1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lkQKTDBb8r5t","colab_type":"text"},"source":["### Training example"]},{"cell_type":"markdown","metadata":{"id":"sO3Xigjy8r5t","colab_type":"text"},"source":["Our training example is made of the vector representing the context words \"i am because i\", and the target which is the one-hot vector representing the center word \"happy\"."]},{"cell_type":"code","metadata":{"id":"caYtN_Ch8r5t","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745166127,"user_tz":-480,"elapsed":689,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["training_examples = get_training_example(words, 2, word2Ind, V)"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZkWMxIY8r5w","colab_type":"text"},"source":["> `get_training_examples`, which uses the `yield` keyword, is known as a generator. When run, it builds an iterator, which is a special type of object that we can iterate on (using a `for` loop for instance), to retrieve the successive values that the function generates.\n",">\n","> In this case `get_training_examples` `yield`s training examples, and iterating on `training_examples` will return the successive training examples."]},{"cell_type":"code","metadata":{"id":"IIF3PN878r5w","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745167211,"user_tz":-480,"elapsed":625,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["x_array, y_array = next(training_examples)"],"execution_count":106,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gmNDuRFX8r5y","colab_type":"text"},"source":["> `next` is another special keyword, which gets the next available value from an iterator. Here, we'll get the very first value, which is the first training example. If we run this cell again, we'll get the next value, and so on until the iterator runs out of values to return.\n"]},{"cell_type":"markdown","metadata":{"id":"luDoniWx8r5y","colab_type":"text"},"source":["The vector representing the context words, which will be fed into the neural network, is:"]},{"cell_type":"code","metadata":{"id":"LYwpHyYE8r5y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745170555,"user_tz":-480,"elapsed":700,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"5518e3b7-7825-43f3-9e82-322bc828a63c"},"source":["x_array"],"execution_count":107,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.25, 0.25, 0.  , 0.5 , 0.  ])"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"markdown","metadata":{"id":"k-cOUfSD8r51","colab_type":"text"},"source":["The one-hot vector representing the center word to be predicted is:"]},{"cell_type":"code","metadata":{"id":"MLOBshrh8r51","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745172910,"user_tz":-480,"elapsed":738,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"a67417c2-a3eb-4087-809f-e5daa6767dc0"},"source":["y_array"],"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 1., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"markdown","metadata":{"id":"-nR-hG548r53","colab_type":"text"},"source":["Now we will convert these vectors into matrices (or 2D arrays) to be able to perform matrix multiplication on the right types of objects."]},{"cell_type":"code","metadata":{"id":"f9yk-gOo8r54","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1594745177937,"user_tz":-480,"elapsed":724,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"8e4d45de-b3f2-4718-c588-38047343855e"},"source":["x = x_array.copy()\n","x.shape = (V, 1)\n","print('x')\n","print(x)\n","print()\n","\n","y = y_array.copy()\n","y.shape = (V, 1)\n","print('y')\n","print(y)"],"execution_count":109,"outputs":[{"output_type":"stream","text":["x\n","[[0.25]\n"," [0.25]\n"," [0.  ]\n"," [0.5 ]\n"," [0.  ]]\n","\n","y\n","[[0.]\n"," [0.]\n"," [1.]\n"," [0.]\n"," [0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FghUHmpY8r56","colab_type":"text"},"source":["### Values of the hidden layer\n","\n","Now that we have initialized all the variables that we need for forward propagation, we can calculate the values of the hidden layer using the following formulas:\n","\n","\\begin{align}\n"," \\mathbf{z_1} = \\mathbf{W_1}\\mathbf{x} + \\mathbf{b_1}  \\\\\n"," \\mathbf{h} = \\mathrm{ReLU}(\\mathbf{z_1})  \\\\\n","\\end{align}\n","\n","First, we can calculate the value of $\\mathbf{z_1}$."]},{"cell_type":"code","metadata":{"id":"H75wc6aL8r56","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745182533,"user_tz":-480,"elapsed":656,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["z1 = np.dot(W1, x) + b1"],"execution_count":110,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rAskQGK38r57","colab_type":"text"},"source":["> `np.dot` is numpy's function for matrix multiplication.\n","\n","As expected we get an $N$ by 1 matrix, or column vector with $N$ elements, where $N$ is equal to the embedding size, which is 3 in this example."]},{"cell_type":"code","metadata":{"id":"issSJTcX8r58","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745184769,"user_tz":-480,"elapsed":792,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"6f2dfecd-247e-4abb-a8cd-089d4293d948"},"source":["z1"],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.36483875],\n","       [ 0.63710329],\n","       [-0.3236647 ]])"]},"metadata":{"tags":[]},"execution_count":111}]},{"cell_type":"markdown","metadata":{"id":"OarpvkJV8r59","colab_type":"text"},"source":["WE can now take the ReLU of $\\mathbf{z_1}$ to get $\\mathbf{h}$, the vector with the values of the hidden layer."]},{"cell_type":"code","metadata":{"id":"4UBJehka8r59","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745185854,"user_tz":-480,"elapsed":539,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"ede8e132-f86a-4121-f2a6-98a6683a7d82"},"source":["h = relu(z1)\n","h"],"execution_count":112,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.36483875],\n","       [0.63710329],\n","       [0.        ]])"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"markdown","metadata":{"id":"Sj-d30ae8r5-","colab_type":"text"},"source":["Applying ReLU means that the negative element of $\\mathbf{z_1}$ has been replaced with a zero."]},{"cell_type":"markdown","metadata":{"id":"CDAui0Nv8r5_","colab_type":"text"},"source":["### Values of the output layer\n","\n","Here are the formulas we need to calculate the values of the output layer, represented by the vector $\\mathbf{\\hat y}$:\n","\n","\\begin{align}\n"," \\mathbf{z_2} &= \\mathbf{W_2}\\mathbf{h} + \\mathbf{b_2}   \\\\\n"," \\mathbf{\\hat y} &= \\mathrm{softmax}(\\mathbf{z_2})   \\\\\n","\\end{align}\n","\n","First, we will calculate $\\mathbf{z_2}$."]},{"cell_type":"code","metadata":{"id":"k4hkmwIh8r5_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745188601,"user_tz":-480,"elapsed":731,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"3cc1465d-b0f1-468f-eb69-09ac3f92c1a2"},"source":["z2 = np.dot(W2, h) + b2\n","z2"],"execution_count":113,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.31973737],\n","       [-0.28125477],\n","       [-0.09838369],\n","       [-0.33512159],\n","       [-0.19919612]])"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"markdown","metadata":{"id":"rK6n9Wza8r6C","colab_type":"text"},"source":["This is a $V$ by 1 matrix, where $V$ is the size of the vocabulary, which is 5 in this example."]},{"cell_type":"markdown","metadata":{"id":"JV46ktIH8r6D","colab_type":"text"},"source":["Now we will calculate the value of $\\mathbf{\\hat y}$."]},{"cell_type":"code","metadata":{"id":"QzBbSOPx8r6D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745295083,"user_tz":-480,"elapsed":752,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"8182e884-c9d3-436f-cff2-9358590c1d20"},"source":["y_hat = softmax(z2)\n","y_hat"],"execution_count":114,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.18519074],\n","       [0.19245626],\n","       [0.23107446],\n","       [0.18236353],\n","       [0.20891502]])"]},"metadata":{"tags":[]},"execution_count":114}]},{"cell_type":"markdown","metadata":{"id":"c09yOyZI8r6E","colab_type":"text"},"source":["As we've performed the calculations with random matrices and vectors (apart from the input vector), the output of the neural network is essentially random at this point. The learning process will adjust the weights and biases to match the actual targets better."]},{"cell_type":"markdown","metadata":{"id":"Q103vvi88r6F","colab_type":"text"},"source":["#### **Cross-entropy loss**\n","\n","Now that we have the network's prediction, we can calculate the cross-entropy loss to determine how accurate the prediction was compared to the actual target.\n","\n","> As we are working on a single training example, and not on a batch of examples, therefore we are using *loss* and not *cost*, which is the generalized form of loss."]},{"cell_type":"code","metadata":{"id":"P8SVQwWz8r6F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745420033,"user_tz":-480,"elapsed":788,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"3dc94f54-743a-4063-a6d2-1339c479fd08"},"source":["y_hat"],"execution_count":115,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.18519074],\n","       [0.19245626],\n","       [0.23107446],\n","       [0.18236353],\n","       [0.20891502]])"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"BcHECtYt8r6H","colab_type":"text"},"source":["And the actual target value is:"]},{"cell_type":"code","metadata":{"id":"OTMteqst8r6H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745422363,"user_tz":-480,"elapsed":826,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"323f8efc-8e72-4c58-e733-710fa72ca791"},"source":["y"],"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [0.],\n","       [1.],\n","       [0.],\n","       [0.]])"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"markdown","metadata":{"id":"d_NZ_sx68r6J","colab_type":"text"},"source":["The formula for cross-entropy loss is:\n","\n","$$ J=-\\sum\\limits_{k=1}^{V}y_k\\log{\\hat{y}_k} $$\n","\n","We will now implement the cross-entropy loss function."]},{"cell_type":"code","metadata":{"id":"zTVsSrc-8r6K","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745456050,"user_tz":-480,"elapsed":785,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def cross_entropy_loss(y_predicted, y_actual):\n","    loss = np.sum(-np.log(y_hat)*y)\n","    \n","    return loss"],"execution_count":117,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"288BjxiS8r6L","colab_type":"text"},"source":["Now let's use this function to calculate the loss with the actual values of $\\mathbf{y}$ and $\\mathbf{\\hat y}$."]},{"cell_type":"code","metadata":{"id":"TYnXezva8r6L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1594745469777,"user_tz":-480,"elapsed":825,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4d9265af-b6cf-4212-eff5-15da1c0e8be7"},"source":["cross_entropy_loss(y_hat, y)"],"execution_count":118,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.4650152923611106"]},"metadata":{"tags":[]},"execution_count":118}]},{"cell_type":"markdown","metadata":{"id":"Vg8nrRID8r6M","colab_type":"text"},"source":["This value is neither good nor bad, which is expected as the neural network hasn't learned anything yet.\n","\n","The actual learning will start during the next phase: backpropagation.\n","\n","#### **Backpropagation**\n","\n","The formulas that we will implement for backpropagation are the following:\n","\n","\\begin{align}\n"," \\frac{\\partial J}{\\partial \\mathbf{W_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top \\\\\n"," \\frac{\\partial J}{\\partial \\mathbf{W_2}} &= (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} \\\\\n"," \\frac{\\partial J}{\\partial \\mathbf{b_1}} &= \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\\\\n"," \\frac{\\partial J}{\\partial \\mathbf{b_2}} &= \\mathbf{\\hat{y}} - \\mathbf{y} \n","\\end{align}\n","\n","\n","Let's start with the easiest one.\n","\n","We will calculate the partial derivative of the loss function with respect to $\\mathbf{b_2}$, and store the result in `grad_b2`.\n","\n","$$\\frac{\\partial J}{\\partial \\mathbf{b_2}} = \\mathbf{\\hat{y}} - \\mathbf{y} $$"]},{"cell_type":"code","metadata":{"id":"8T9LFGJy8r6N","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745569319,"user_tz":-480,"elapsed":749,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"dd2983d9-0865-424a-dd5e-52e68f00bf0d"},"source":["grad_b2 = y_hat - y\n","grad_b2"],"execution_count":119,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.18519074],\n","       [ 0.19245626],\n","       [-0.76892554],\n","       [ 0.18236353],\n","       [ 0.20891502]])"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"markdown","metadata":{"id":"qV6AT3gl8r6O","colab_type":"text"},"source":["Next, we will calculate the partial derivative of the loss function with respect to $\\mathbf{W_2}$, and store the result in `grad_W2`.\n","\n","$$\\frac{\\partial J}{\\partial \\mathbf{W_2}} = (\\mathbf{\\hat{y}} - \\mathbf{y})\\mathbf{h^\\top} $$"]},{"cell_type":"code","metadata":{"id":"UMc2Ck_n8r6O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745603923,"user_tz":-480,"elapsed":707,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"38d8d0c1-38a8-4267-a4f4-78e3f8437378"},"source":["grad_W2 = np.dot(y_hat - y, h.T)\n","grad_W2"],"execution_count":120,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.06756476,  0.11798563,  0.        ],\n","       [ 0.0702155 ,  0.12261452,  0.        ],\n","       [-0.28053384, -0.48988499,  0.        ],\n","       [ 0.06653328,  0.1161844 ,  0.        ],\n","       [ 0.07622029,  0.13310045,  0.        ]])"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"id":"pq3-p-Ad8r6P","colab_type":"text"},"source":["Now we will calculate the partial derivative with respect to $\\mathbf{b_1}$ and store the result in `grad_b1`.\n","\n","$$\\frac{\\partial J}{\\partial \\mathbf{b_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right ) $$"]},{"cell_type":"code","metadata":{"id":"GR2QgTOA8r6P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745628401,"user_tz":-480,"elapsed":701,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"e89e26d6-c664-4f01-b809-6a4ce2ff9212"},"source":["grad_b1 = relu(np.dot(W2.T, y_hat - y))\n","grad_b1"],"execution_count":121,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        ],\n","       [0.        ],\n","       [0.17045858]])"]},"metadata":{"tags":[]},"execution_count":121}]},{"cell_type":"markdown","metadata":{"id":"9rnJ4aOC8r6R","colab_type":"text"},"source":["Finally, we will calculate the partial derivative of the loss with respect to $\\mathbf{W_1}$, and store it in `grad_W1`.\n","\n","$$\\frac{\\partial J}{\\partial \\mathbf{W_1}} = \\rm{ReLU}\\left ( \\mathbf{W_2^\\top} (\\mathbf{\\hat{y}} - \\mathbf{y})\\right )\\mathbf{x}^\\top $$"]},{"cell_type":"code","metadata":{"id":"9QaMOBnW8r6S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745655691,"user_tz":-480,"elapsed":735,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"82ee5a1c-1fbc-4ec2-f5f4-a8c1aa3c203b"},"source":["grad_W1 = np.dot(relu(np.dot(W2.T, y_hat - y)), x.T)\n","grad_W1"],"execution_count":122,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.        , 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 0.        ],\n","       [0.04261464, 0.04261464, 0.        , 0.08522929, 0.        ]])"]},"metadata":{"tags":[]},"execution_count":122}]},{"cell_type":"code","metadata":{"id":"mVCyGm2d8r6W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1594745679292,"user_tz":-480,"elapsed":796,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"e2ead8fe-c11d-4fd2-c8a6-bc88185a0abe"},"source":["print(f'V (vocabulary size): {V}')\n","print(f'N (embedding size / size of the hidden layer): {N}')\n","print(f'size of grad_W1: {grad_W1.shape} (NxV)')\n","print(f'size of grad_b1: {grad_b1.shape} (Nx1)')\n","print(f'size of grad_W2: {grad_W1.shape} (VxN)')\n","print(f'size of grad_b2: {grad_b2.shape} (Vx1)')"],"execution_count":123,"outputs":[{"output_type":"stream","text":["V (vocabulary size): 5\n","N (embedding size / size of the hidden layer): 3\n","size of grad_W1: (3, 5) (NxV)\n","size of grad_b1: (3, 1) (Nx1)\n","size of grad_W2: (3, 5) (VxN)\n","size of grad_b2: (5, 1) (Vx1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Anhsv-c88r6X","colab_type":"text"},"source":["#### **Gradient descent**\n","\n","During the gradient descent phase, we will update the weights and biases by subtracting $\\alpha$ times the gradient from the original matrices and vectors, using the following formulas.\n","\n","\\begin{align}\n"," \\mathbf{W_1} &:= \\mathbf{W_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_1}} \\\\\n"," \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\\\\n"," \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\\\\n"," \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\\\\n","\\end{align}\n","\n","First, let set a value for $\\alpha$."]},{"cell_type":"code","metadata":{"id":"38z5zyIU8r6X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745731374,"user_tz":-480,"elapsed":776,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["alpha = 0.03"],"execution_count":124,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iZzzUNbb8r6Z","colab_type":"text"},"source":["The updated weight matrix $\\mathbf{W_1}$ will be:"]},{"cell_type":"code","metadata":{"id":"zMlwUZ3W8r6a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594745733254,"user_tz":-480,"elapsed":724,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["W1_new = W1 - alpha * grad_W1"],"execution_count":125,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i7S2ii1W8r6c","colab_type":"text"},"source":["Let's compare the previous and new values of $\\mathbf{W_1}$:"]},{"cell_type":"code","metadata":{"id":"qnX82krg8r6c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":177},"executionInfo":{"status":"ok","timestamp":1594745736739,"user_tz":-480,"elapsed":717,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"43583f8a-acb9-41ce-95b9-7bac62199ef4"},"source":["print('old value of W1:')\n","print(W1)\n","print()\n","print('new value of W1:')\n","print(W1_new)"],"execution_count":126,"outputs":[{"output_type":"stream","text":["old value of W1:\n","[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n"," [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n"," [ 0.26637602 -0.23846886 -0.37770863 -0.11399446  0.34008124]]\n","\n","new value of W1:\n","[[ 0.41687358  0.08854191 -0.23495225  0.28320538  0.41800106]\n"," [ 0.32735501  0.22795148 -0.23951958  0.4117634  -0.23924344]\n"," [ 0.26509758 -0.2397473  -0.37770863 -0.11655134  0.34008124]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dAfK4Z4F8r6e","colab_type":"text"},"source":["The difference is very subtle (the last row), which is why it takes a fair amount of iterations to train the neural network until it reaches optimal weights and biases starting from random values.\n","\n","Now we will calculate the new values of $\\mathbf{W_2}$ (to be stored in `W2_new`), $\\mathbf{b_1}$ (in `b1_new`), and $\\mathbf{b_2}$ (in `b2_new`).\n","\n","\\begin{align}\n"," \\mathbf{W_2} &:= \\mathbf{W_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{W_2}} \\\\\n"," \\mathbf{b_1} &:= \\mathbf{b_1} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_1}} \\\\\n"," \\mathbf{b_2} &:= \\mathbf{b_2} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{b_2}} \\\\\n","\\end{align}"]},{"cell_type":"code","metadata":{"id":"MkyFqHuQ8r6e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1594745788755,"user_tz":-480,"elapsed":795,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"7e70c250-7dcc-4586-f76e-14e7b9c09267"},"source":["W2_new = W2 - alpha * grad_W2\n","b1_new = b1 - alpha * grad_b1\n","b2_new = b2 - alpha * grad_b2\n","\n","print('W2_new')\n","print(W2_new)\n","print()\n","print('b1_new')\n","print(b1_new)\n","print()\n","print('b2_new')\n","print(b2_new)"],"execution_count":127,"outputs":[{"output_type":"stream","text":["W2_new\n","[[-0.22384758 -0.43362588  0.13310965]\n"," [ 0.08265956  0.0775535   0.1772054 ]\n"," [ 0.19557112 -0.04637608 -0.1790735 ]\n"," [ 0.06855622 -0.02363691  0.36107434]\n"," [ 0.33251813 -0.3982269  -0.43959196]]\n","\n","b1_new\n","[[ 0.09688219]\n"," [ 0.29239497]\n"," [-0.27875802]]\n","\n","b2_new\n","[[ 0.02964508]\n"," [-0.36970753]\n"," [-0.10468778]\n"," [-0.35349417]\n"," [-0.0764456 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6p8bsLFM8r6f","colab_type":"text"},"source":["#### **Extracting word embedding vectors**\n","\n","We have finished training the neural network. We have three options to get word embedding vectors for the words of our vocabulary, based on the weight matrices $\\mathbf{W_1}$ and/or $\\mathbf{W_2}$.\n","\n","### Option 1: extract embedding vectors from $\\mathbf{W_1}$\n","\n","The first option is to take the columns of $\\mathbf{W_1}$ as the embedding vectors of the words of the vocabulary, using the same order of the words as for the input and output vectors.\n","\n","For example $\\mathbf{W_1}$ is this matrix:"]},{"cell_type":"code","metadata":{"id":"KmLRDUkH8r6f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745857575,"user_tz":-480,"elapsed":706,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"3c77f38e-c735-4eb8-f1ff-f6f401c61db1"},"source":["W1"],"execution_count":128,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n","       [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n","       [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])"]},"metadata":{"tags":[]},"execution_count":128}]},{"cell_type":"markdown","metadata":{"id":"oe2llXUV8r6h","colab_type":"text"},"source":["The first column, which is a 3-element vector, is the embedding vector of the first word of our vocabulary. The second column is the word embedding vector for the second word, and so on.\n","\n","The first, second, etc. words are ordered as follows."]},{"cell_type":"code","metadata":{"id":"-vfoqChC8r6h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745889620,"user_tz":-480,"elapsed":928,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4747d058-09d3-47ca-edfe-4054a7968b72"},"source":["for i in range(V):\n","    print(Ind2word[i])"],"execution_count":129,"outputs":[{"output_type":"stream","text":["am\n","because\n","happy\n","i\n","learning\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y8D-AJ418r6i","colab_type":"text"},"source":["So the word embedding vectors corresponding to each word are:"]},{"cell_type":"code","metadata":{"id":"iRQLAN_38r6i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745901338,"user_tz":-480,"elapsed":1368,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4630aee1-933d-435c-a7a8-7b06bf8d3c8c"},"source":["# loop through each word of the vocabulary\n","for word in word2Ind:\n","    # extract the column corresponding to the index of the word in the vocabulary\n","    word_embedding_vector = W1[:, word2Ind[word]]\n","    \n","    print(f'{word}: {word_embedding_vector}')"],"execution_count":130,"outputs":[{"output_type":"stream","text":["am: [0.41687358 0.32735501 0.26637602]\n","because: [ 0.08854191  0.22795148 -0.23846886]\n","happy: [-0.23495225 -0.23951958 -0.37770863]\n","i: [ 0.28320538  0.4117634  -0.11399446]\n","learning: [ 0.41800106 -0.23924344  0.34008124]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Asq8T2Tq8r6j","colab_type":"text"},"source":["### Option 2: extract embedding vectors from $\\mathbf{W_2}$"]},{"cell_type":"markdown","metadata":{"id":"76HtGy338r6j","colab_type":"text"},"source":["The second option is to take $\\mathbf{W_2}$ transpose, and take its columns as the word embedding vectors just like we did for $\\mathbf{W_1}$."]},{"cell_type":"code","metadata":{"id":"cehKurFv8r6j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745934475,"user_tz":-480,"elapsed":1203,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"7af29946-1557-4bfa-ca48-9b8a04a7f576"},"source":["W2.T"],"execution_count":131,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.22182064,  0.08476603,  0.1871551 ,  0.07055222,  0.33480474],\n","       [-0.43008631,  0.08123194, -0.06107263, -0.02015138, -0.39423389],\n","       [ 0.13310965,  0.1772054 , -0.1790735 ,  0.36107434, -0.43959196]])"]},"metadata":{"tags":[]},"execution_count":131}]},{"cell_type":"code","metadata":{"id":"Uk2kO5hi8r6l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594745937292,"user_tz":-480,"elapsed":984,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"bce0c478-e6c7-4241-e4a8-102e2b329be9"},"source":["# loop through each word of the vocabulary\n","for word in word2Ind:\n","    # extract the column corresponding to the index of the word in the vocabulary\n","    word_embedding_vector = W2.T[:, word2Ind[word]]\n","    \n","    print(f'{word}: {word_embedding_vector}')"],"execution_count":132,"outputs":[{"output_type":"stream","text":["am: [-0.22182064 -0.43008631  0.13310965]\n","because: [0.08476603 0.08123194 0.1772054 ]\n","happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n","i: [ 0.07055222 -0.02015138  0.36107434]\n","learning: [ 0.33480474 -0.39423389 -0.43959196]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yYkdBVts8r6m","colab_type":"text"},"source":["### Option 3: extract embedding vectors from $\\mathbf{W_1}$ and $\\mathbf{W_2}$"]},{"cell_type":"markdown","metadata":{"id":"u13ZF8JY8r6m","colab_type":"text"},"source":["The third option uses the average of $\\mathbf{W_1}$ and $\\mathbf{W_2^\\top}$."]},{"cell_type":"code","metadata":{"id":"G0FPYR298r6n","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594745975544,"user_tz":-480,"elapsed":727,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"5fbeaaa7-a518-4a78-cd93-8cf8aaa9abbd"},"source":["W3 = (W1+W2.T)/2\n","W3"],"execution_count":133,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.09752647,  0.08665397, -0.02389858,  0.1768788 ,  0.3764029 ],\n","       [-0.05136565,  0.15459171, -0.15029611,  0.19580601, -0.31673866],\n","       [ 0.19974284, -0.03063173, -0.27839106,  0.12353994, -0.04975536]])"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"markdown","metadata":{"id":"-Jzl1Wff8r6o","colab_type":"text"},"source":["Extracting the word embedding vectors works just like the two previous options, by taking the columns of the matrix we have just created."]},{"cell_type":"code","metadata":{"id":"U2oQmb_i8r6o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1594746003862,"user_tz":-480,"elapsed":663,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"29f6b1e2-63a4-4902-f813-72629034ac15"},"source":["# loop through each word of the vocabulary\n","for word in word2Ind:\n","    # extract the column corresponding to the index of the word in the vocabulary\n","    word_embedding_vector = W3[:, word2Ind[word]]    \n","    print(f'{word}: {word_embedding_vector}')"],"execution_count":135,"outputs":[{"output_type":"stream","text":["am: [ 0.09752647 -0.05136565  0.19974284]\n","because: [ 0.08665397  0.15459171 -0.03063173]\n","happy: [-0.02389858 -0.15029611 -0.27839106]\n","i: [0.1768788  0.19580601 0.12353994]\n","learning: [ 0.3764029  -0.31673866 -0.04975536]\n"],"name":"stdout"}]}]}